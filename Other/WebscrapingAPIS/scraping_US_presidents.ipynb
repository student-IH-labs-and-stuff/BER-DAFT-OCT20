{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK \n",
    "\n",
    "Scrape this list of presidents of the United States.\n",
    "\n",
    "Collect all the links to the Wikipedia page of each president.\n",
    "\n",
    "Scrape the Wikipedia page of each president.\n",
    "\n",
    "Find and store information about each president.\n",
    "\n",
    "Organize the information in a dataframe where we have each president as a row and each variable we collected as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. find url and store it in a variable\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# 4.2. check that the html code looks like it should\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to pick up all presidents names ? \n",
    "#hint - look at CSS selector nth-child  or use pandas html reader\n",
    "\n",
    "presidents = []\n",
    "\n",
    "#your code\n",
    "\n",
    "# check the output:\n",
    "presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to pick up all presidents websites ? \n",
    "#hint - look at CSS selector nth-child  or use pandas html reader\n",
    "\n",
    "presidentspg = []\n",
    "\n",
    "#your code \n",
    "\n",
    "# check the output \n",
    "\n",
    "presidentspg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get hold of some facts about the presidents- from the websites for each president linked to their name on the wiki page, such as:\n",
    "\n",
    "date of birth\n",
    "\n",
    "period in office\n",
    "\n",
    "spouse name \n",
    "\n",
    "no of children \n",
    "\n",
    "educated at \n",
    "\n",
    "\n",
    "TIP - when scraping data from multiple pages that are similar - start with a couple of pages individually to form and test your code \n",
    "\n",
    "on the presidents pages you will spot an info box on the right - you can scrape some useful info from there about each president\n",
    "https://en.wikipedia.org/wiki/John_Adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code to test one site \n",
    "\n",
    "# send request\n",
    "url = \"https://en.wikipedia.org/\" + presidentspg[0][\"href\"]\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "# parse & store html\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup.find(\"table\", { \"class\" : \"infobox vcard\"})\n",
    "\n",
    "#your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have managed to make this code work and we tested it for a few presidents, it's time to create the loop that will send all the requests.\n",
    "\n",
    "In this step you could very well store the whole wikipedia page for each president, or just the tiny, final pieces of information. Storing the boxes is a middle ground (we don't have too much noise but retain the flexibility of deciding later which specific elements to extract).\n",
    "\n",
    "When sending multiple requests, remember to be respectful by spacing the requests a few seconds from each other. \n",
    "\n",
    "PRO TIP - you should  also print the success code to monitor that everything is going well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code to grab limited html data from all president sites -fact box\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for future ref in this notebook, I will refer to this as 'presi_soups'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, find the facts from each fact box per president - You should ideally test what you can get from a single presidents and then assemble a loop for all of them - as usual.\n",
    "\n",
    "TIP - use the string argument in a find function sometimes, since wikipedia tags and classes are not always helpful to locate the facts you want in the html. The string argument allows you to locate elements by its actual content.\n",
    "\n",
    "\n",
    "eg.#Political party\n",
    "\n",
    "presi_soups[-1].find(\"th\", string=\"Political party\").parent.find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning - on some fact hunts, ERRORS can occur \n",
    "# eg what if one president doesnt have children?\n",
    "\n",
    "children = []\n",
    "\n",
    "for presi in presi_soups:\n",
    "    children.append(len(presi.find(\"th\", string=\"Children\").parent.find_all(\"li\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the error - copy paste it in the below error frame for handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one way to handle this error - not best practice \n",
    "#(this is a blunt instrument until the error is specified)\n",
    "# modify this error handling script for your particular error\n",
    "\n",
    "#https://stackoverflow.com/questions/53940768/web-scraping-error-handling-when-web-page-doesnt-contain-any-specific-element\n",
    "\n",
    "for i in presi_soups: \n",
    "    try:\n",
    "        print(i.find(\"th\", strings=\"Children\").parent.find_all(\"li\"))\n",
    "    except: \n",
    "        print(\"NA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
