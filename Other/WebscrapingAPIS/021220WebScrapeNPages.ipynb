{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. url: we start with the 'second' page\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# 4.2. check that the html code looks like it should\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(1, 631, 50)\n",
    "\n",
    "for i in iterations:\n",
    "    start_at= str(i)\n",
    "    url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=\" + start_at + \"&ref_=adv_nxt\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respectful scraping:\n",
    "\n",
    "There we have it, all the URLs we need! Before starting with the actual scraping, though, there's something we need to note when sending massive, automated requests to websites: it's rude. \n",
    "\n",
    "We just have 13 of them, which is not too many, but it's still a good practice to let a few seconds pass in between requests. \n",
    "\n",
    "Some pages don't like being scraped and will block your IP if they detect it's sending automated requests. Others might have a small server for the traffic they handle, and sending too many requests might crash the site. The sleep module will help us with that. \n",
    "\n",
    "Here's how it works, waiting 2 seconds between each iteration in a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need a few more tools for this one \n",
    "from random import randint\n",
    "import time\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic use of sleep in for loop \n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more human use of sleep \n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    wait_time = randint(1,4)\n",
    "    print(\"I will sleep for \" + str(wait_time) + \" seconds.\")\n",
    "    time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make this more interactive \n",
    "- you can split the below scripts out into separate cells and run them yourself or re-write them in your own python style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembling the script to send and store multiple requests\n",
    "\n",
    "pages = []\n",
    "\n",
    "for i in iterations:\n",
    "    # assemble the url:\n",
    "    start_at= str(i)\n",
    "    url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=\" + start_at + \"&ref_=adv_nxt\"\n",
    "\n",
    "    # download html with a get request:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # monitor the process by printing the status code\n",
    "    print(\"Status code: \" + str(response.status_code))\n",
    "\n",
    "    # store response into \"pages\" list\n",
    "    pages.append(response)\n",
    "\n",
    "    # respectful nap:\n",
    "    wait_time = randint(1,4)\n",
    "    print(\"I will sleep for \" + str(wait_time) + \" second/s.\")\n",
    "    time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the object pages after running the code above, you'll just see the response code messages, but the html code is still accessible and you can parse it the same way we've always done:\n",
    "\n",
    "BeautifulSoup(pages[0].content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse just the first page, for testing purposes\n",
    "soup = BeautifulSoup(pages[0].content, \"html.parser\")\n",
    "\n",
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select(\"#main > div > div.lister.list.detail.sub-list > div > div:nth-child(1) > div.lister-item-content > h3 > a\")\n",
    "\n",
    "# Trim the selection: now it grabs all the titles\n",
    "soup.select(\"div.lister-item-content > h3 > a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select(\"#main > div > div.lister.list.detail.sub-list > div > div:nth-child(1) > div.lister-item-content  > p:nth-child(4)\")\n",
    "\n",
    "# Trim the selection: now it grabs all the titles\n",
    "soup.select(\"div.lister-item-content > p:nth-child(4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ugliest things about the code above is that the HTML element containing the synopsis does not have any combination of tag and attribute that makes it unique. We've had to use select(\"p:nth-child(4)\") and simply grab the 4th <p> element. Not very elegant... potentially will break... but, for now, it works.\n",
    "\n",
    "We have noticed how both the title and the synopsis are children of div.lister-item-content. That will make our looping task a bit simpler.\n",
    "\n",
    "There are many approaches to do this. The one we'll follow is:\n",
    "\n",
    "Loop through the pages we collected, parse them (\"create the soup\") and store the parsed pages in a list.\n",
    "For each parsed page, select the \"blocks of HTML elements\" that contain all the information of each movie (the title, the synopsis and other stuff).\n",
    "For each one of the \"blocks\" we collected in the previous step:\n",
    "Get the movie titles and store them in a list\n",
    "Get the synopsis and store them in a list\n",
    "Here's the code that does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_parsed = []\n",
    "titles = []\n",
    "synopsis = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    # parse all pages\n",
    "    pages_parsed.append(BeautifulSoup(pages[i].content, \"html.parser\"))\n",
    "    # select only the info about the movies\n",
    "    movies_html = pages_parsed[i].select(\"div.lister-item-content\")\n",
    "    # for movie, store titles and reviews into lists\n",
    "    for j in range(len(movies_html)):\n",
    "        titles.append(movies_html[j].select(\"h3 > a\")[0].get_text())\n",
    "        synopsis.append(movies_html[j].select(\"p:nth-child(4)\")[0].get_text().strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
